try:
    import sympy, sys, subprocess
    from packaging import version
    if version.parse(sympy.__version__) >= version.parse("1.13"):
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "sympy==1.12"])
        import importlib; import sympy as _sym; importlib.reload(_sym)
        print("Pinned SymPy to:", _sym.__version__)
except Exception as e:
    print("SymPy pin note:", repr(e))

import math, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F
from typing import List, Dict, Optional, Tuple

# ----------------------------
# Small helpers
# ----------------------------
def _g(seed=None):
    g = torch.Generator()
    if seed is not None:
        g.manual_seed(seed)
    return g

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# ----------------------------
# Nonlinear SEM (Diamond model)
# w ~ U(-3,3)
# x = w^2 + n_x,  n_x ~ U(-1,1)
# y = 4*sqrt(|w|) + n_y,  n_y ~ U(-1,1)
# z = 2*sin(x) + 2*sin(y) + n_z,  n_z ~ U(-1,1)
# Mixed interventions per prompt:
#   - Mean: +delta on selected entries (no sign sampling)
#   - Variance: widen half-widths by alpha = 1 + v (mean unchanged)
# ----------------------------
VAR_NAMES = ["w","x","y","z"]
P = 4

def sample_diamond_mixed(n: int,
                         delta: Optional[torch.Tensor] = None,
                         alpha: Optional[torch.Tensor] = None,
                         seed: Optional[int] = None) -> torch.Tensor:
    """
    Returns X of shape (n,4): [w,x,y,z].
    delta: length-4 mean shifts applied to w (direct) and to noises of x,y,z; default zeros.
    alpha: length-4 width scales (alpha >= 1 ideally). alpha=1 => baseline widths.
           - For w: baseline Unif(-3,3) -> Unif(-3*alpha_w, 3*alpha_w)
           - For n_x,n_y,n_z: baseline Unif(-1,1) -> Unif(-alpha, alpha)
    """
    g = _g(seed)
    if delta is None:
        delta = torch.zeros(P)
    if alpha is None:
        alpha = torch.ones(P)
    delta = delta.to(torch.float32)
    alpha = alpha.to(torch.float32)

    # root w
    w_noise = (torch.rand(n, generator=g) * (6.0 * alpha[0]) - (3.0 * alpha[0])).to(torch.float32)
    w = w_noise + delta[0]

    # x
    nx = (torch.rand(n, generator=g) * (2.0 * alpha[1]) - (1.0 * alpha[1])).to(torch.float32)
    x = (w ** 2) + (nx + delta[1])

    # y
    ny = (torch.rand(n, generator=g) * (2.0 * alpha[2]) - (1.0 * alpha[2])).to(torch.float32)
    y = 4.0 * torch.sqrt(torch.clamp(w.abs(), min=1e-12)) + (ny + delta[2])

    # z
    nz = (torch.rand(n, generator=g) * (2.0 * alpha[3]) - (1.0 * alpha[3])).to(torch.float32)
    z = 2.0 * torch.sin(x) + 2.0 * torch.sin(y) + (nz + delta[3])

    return torch.stack([w, x, y, z], dim=1)

def empirical_stats(X_obs: torch.Tensor, X_int: torch.Tensor, eps_reg: float = 1e-3):
    mu_obs = X_obs.mean(dim=0)
    mu_int = X_int.mean(dim=0)
    Xm = X_obs - mu_obs
    Xi = X_int - mu_int
    p = X_obs.shape[1]
    eye = torch.eye(p, dtype=X_obs.dtype, device=X_obs.device)
    Sigma_obs = (Xm.T @ Xm) / max(1, X_obs.shape[0] - 1) + eps_reg * eye
    Sigma_int = (Xi.T @ Xi) / max(1, X_int.shape[0] - 1) + eps_reg * eye
    return mu_obs, mu_int, Sigma_obs, Sigma_int

# ----------------------------
# Token encoder (NO L_hat)
# Token_i = [mu_o[i], mu_i[i], dmu[i], log var_o[i], log var_i[i],
#            (Sigma_o @ P_obs)[i,:], (Sigma_i @ P_int)[i,:], ((Sigma_i - Sigma_o) @ P_delta)[i,:]]
# ----------------------------
class TokenEncoderMixedNoL(nn.Module):
    def __init__(self, p: int, r_sigma: int, d_model: int, mlp_hidden: int, seed: int = 1234):
        super().__init__()
        self.p = p
        g = _g(seed)
        P_obs   = torch.randn(p, r_sigma, generator=g) / math.sqrt(p)
        P_int   = torch.randn(p, r_sigma, generator=g) / math.sqrt(p)
        P_delta = torch.randn(p, r_sigma, generator=g) / math.sqrt(p)
        self.register_buffer("P_obs",   P_obs)
        self.register_buffer("P_int",   P_int)
        self.register_buffer("P_delta", P_delta)

        base_dim = 3 + 2 + 3*r_sigma  # [mu_o, mu_i, dmu] + [logvar_o, logvar_i] + three compressed rows
        self.proj = nn.Sequential(
            nn.Linear(base_dim, mlp_hidden),
            nn.GELU(),
            nn.Linear(mlp_hidden, d_model)
        )

    def forward(self, mu_obs, mu_int, Sigma_obs, Sigma_int):
        p = mu_obs.shape[0]
        dmu  = mu_int - mu_obs
        var_obs = torch.diag(Sigma_obs).clamp_min(1e-12)
        var_int = torch.diag(Sigma_int).clamp_min(1e-12)
        logvar_pack = torch.stack([var_obs.log(), var_int.log()], dim=1)  # (p,2)

        comp_obs   = Sigma_obs @ self.P_obs     # (p, r_sigma)
        comp_int   = Sigma_int @ self.P_int     # (p, r_sigma)
        comp_delta = (Sigma_int - Sigma_obs) @ self.P_delta  # (p, r_sigma)

        mu_pack = torch.stack([mu_obs, mu_int, dmu], dim=1)  # (p,3)
        feat = torch.cat([mu_pack, logvar_pack, comp_obs, comp_int, comp_delta], dim=1)  # (p, base_dim)
        X = self.proj(feat)                                  # (p, d_model)
        return X.unsqueeze(0)                                # (1, p, d_model)

# ----------------------------
# Set Transformer blocks
# ----------------------------
class FeedForward(nn.Module):
    def __init__(self, d, d_ff, dropout=0.0):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(d, d_ff), nn.GELU(), nn.Dropout(dropout),
            nn.Linear(d_ff, d)
        )
        self.ln = nn.LayerNorm(d)
    def forward(self, x): return self.ln(x + self.net(x))

class MAB(nn.Module):
    def __init__(self, d, n_heads, d_ff, dropout=0.0):
        super().__init__()
        self.mha = nn.MultiheadAttention(d, n_heads, dropout=dropout, batch_first=True)
        self.ln1 = nn.LayerNorm(d)
        self.ff = FeedForward(d, d_ff, dropout)
    def forward(self, Q, K):
        attn, _ = self.mha(Q, K, K, need_weights=False)
        H = self.ln1(Q + attn)
        return self.ff(H)

class SAB(nn.Module):
    def __init__(self, d, n_heads, d_ff, dropout=0.0):
        super().__init__()
        self.mab = MAB(d, n_heads, d_ff, dropout)
    def forward(self, X): return self.mab(X, X)

class ISAB(nn.Module):
    def __init__(self, d, n_heads, m, d_ff, dropout=0.0):
        super().__init__()
        self.I = nn.Parameter(torch.randn(1, m, d) / math.sqrt(d))
        self.mab1 = MAB(d, n_heads, d_ff, dropout)  # H = MAB(I, X)
        self.mab2 = MAB(d, n_heads, d_ff, dropout)  # Y = MAB(X, H)
    def forward(self, X):
        I = self.I.expand(X.size(0), -1, -1)
        H = self.mab1(I, X)
        return self.mab2(X, H)

class EquivariantBackbone(nn.Module):
    def __init__(self, d, n_heads, d_ff, depth, dropout=0.0, use_isab=True, m_induce=64):
        super().__init__()
        blocks = []
        for _ in range(depth):
            blocks.append(ISAB(d, n_heads, m_induce, d_ff, dropout) if use_isab
                          else SAB(d, n_heads, d_ff, dropout))
        self.blocks = nn.ModuleList(blocks)
    def forward(self, X):
        for b in self.blocks: X = b(X)
        return X

# ----------------------------
# Model (BCE head)
# ----------------------------
class ICLSetTransformerMixedNoL(nn.Module):
    def __init__(self, p: int, d_model: int = 128, n_heads: int = 8, d_ff: int = 256, depth: int = 3,
                 r_sigma: int = 16, mlp_hidden: int = 128, dropout: float = 0.0,
                 use_isab: bool = True, m_induce: int = 64, max_R: int = 128):
        super().__init__()
        self.p = p
        self.encoder = TokenEncoderMixedNoL(p, r_sigma, d_model, mlp_hidden)
        self.backbone = EquivariantBackbone(d_model, n_heads, d_ff, depth, dropout, use_isab, m_induce)
        self.cross = MAB(d_model, n_heads, d_ff, dropout)   # Query ← Supports
        self.role_embed = nn.Embedding(2, d_model)          # 0=support, 1=query
        self.prompt_embed = nn.Embedding(max_R, d_model)    # support id
        self.cls_head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, 1))

    def encode_sem(self, mu_obs, mu_int, Sigma_obs, Sigma_int, role_id: int, prompt_id: Optional[int] = None):
        X = self.encoder(mu_obs, mu_int, Sigma_obs, Sigma_int)  # (1,p,d)
        X = X + self.role_embed.weight[role_id].view(1,1,-1).expand(1, self.p, -1)
        if prompt_id is not None:
            X = X + self.prompt_embed.weight[prompt_id % self.prompt_embed.num_embeddings].view(1,1,-1).expand(1, self.p, -1)
        return X

    def forward_episode(self, supports: List[Dict], query: Dict) -> torch.Tensor:
        S_list = []
        for r, s in enumerate(supports):
            Xs = self.encode_sem(s["mu_obs"], s["mu_int"], s["Sigma_obs"], s["Sigma_int"], role_id=0, prompt_id=r)
            S_list.append(Xs)
        S = torch.cat(S_list, dim=1) if len(S_list) > 0 else None

        Q = self.encode_sem(query["mu_obs"], query["mu_int"], query["Sigma_obs"], query["Sigma_int"], role_id=1)

        if S is not None:
            S = self.backbone(S)
        Q = self.backbone(Q)
        if S is not None:
            Q = self.cross(Q, S)

        logits = self.cls_head(Q).squeeze(0).squeeze(-1)   # (p,)
        return torch.sigmoid(logits)

# ----------------------------
# Build a single prompt (mixed: mean OR variance)
#   - If type='mean': delta[idx] = +mag_mu (no sign sampling), alpha=1
#   - If type='var' : alpha[idx] = 1 + mag_sig (width scale), delta=0
# ----------------------------
def build_prompt_mixed(s: int, n_obs: int, n_int: int,
                       mag_mu: float, mag_sig: float,
                       seed: int, prompt_type: Optional[str] = None,
                       prob_mean: float = 0.5) -> Dict:
    g = _g(seed)
    idx = torch.randperm(P, generator=g)[:s]
    y = torch.zeros(P); y[idx] = 1.0

    if prompt_type is None:
        prompt_type = "mean" if (torch.rand(1, generator=g).item() < prob_mean) else "var"

    if prompt_type == "mean":
        delta = torch.zeros(P); delta[idx] = float(mag_mu)
        alpha = torch.ones(P)
    else:  # 'var'
        delta = torch.zeros(P)
        alpha = torch.ones(P); alpha[idx] = 1.0 + float(mag_sig)

    X_obs = sample_diamond_mixed(n_obs, delta=None, alpha=None, seed=seed)
    X_int = sample_diamond_mixed(n_int, delta=delta, alpha=alpha, seed=seed+1)

    mu_obs, mu_int, Sigma_obs, Sigma_int = empirical_stats(X_obs, X_int, eps_reg=1e-3)
    out = {"type": prompt_type, "y": y, "delta": delta, "alpha": alpha,
           "mu_obs": mu_obs, "mu_int": mu_int, "Sigma_obs": Sigma_obs, "Sigma_int": Sigma_int}
    return out

def make_episode_mixed(R: int, s: int, n_obs: int, n_int: int,
                       mag_mu: float, mag_sig: float, seed: int,
                       prob_mean_supports: float = 0.5, prob_mean_query: Optional[float] = None):
    if prob_mean_query is None:
        prob_mean_query = prob_mean_supports
    supports = [build_prompt_mixed(s, n_obs, n_int, mag_mu, mag_sig,
                                   seed + 100*r, prompt_type=None, prob_mean=prob_mean_supports)
                for r in range(R)]
    # random query type under prob_mean_query
    query = build_prompt_mixed(s, n_obs, n_int, mag_mu, mag_sig,
                               seed + 9999, prompt_type=None, prob_mean=prob_mean_query)
    return supports, query

# ----------------------------
# Training (BCE only)
# ----------------------------
def train_bce_mixed(model: nn.Module, steps: int, R: int, s: int, n_obs: int, n_int: int,
                    mag_mu_choices=(0.2,0.4,0.6,0.8,1.0), mag_sig_choices=(0.2,0.4,0.6,0.8,1.0),
                    prob_mean_supports: float = 0.5, lr=2e-4, wd=1e-4):
    model.to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(1, steps))

    for t in range(steps):
        mag_mu  = float(np.random.choice(mag_mu_choices))
        mag_sig = float(np.random.choice(mag_sig_choices))
        supports, query = make_episode_mixed(R, s, n_obs, n_int, mag_mu, mag_sig, seed=t,
                                             prob_mean_supports=prob_mean_supports)

        for spt in supports:
            for k in ["mu_obs","mu_int","Sigma_obs","Sigma_int","y"]:
                spt[k] = spt[k].to(device)
        for k in ["mu_obs","mu_int","Sigma_obs","Sigma_int","y"]:
            query[k] = query[k].to(device)

        p_q = model.forward_episode(supports, query)
        loss = F.binary_cross_entropy(p_q.clamp(1e-6, 1-1e-6), query["y"])

        opt.zero_grad(); loss.backward(); opt.step(); sched.step()
        if (t+1) % 10 == 0:
            print(f"step {t+1:04d} | BCE={loss.item():.4f}")
    return model

# ----------------------------
# Paired Evaluation (mean & var query per grid cell)
# ----------------------------
@torch.no_grad()
def eval_grid_mixed(model, R, s, n_obs, n_int, mag_mu_values, mag_sig_values,
                    trials=20, k_top=5, prob_mean_supports=0.5, print_progress=True):
    summary_rows, details_rows = [], []
    p = P
    device_ = next(model.parameters()).device

    total_cells = len(mag_mu_values) * len(mag_sig_values)
    cell_idx = 0
    if print_progress:
        print(f"Evaluating {total_cells} grid cells ({len(mag_mu_values)}×{len(mag_sig_values)}), trials={trials}")

    for mag_mu in mag_mu_values:
        for mag_sig in mag_sig_values:
            cell_idx += 1
            if print_progress:
                print(f"[grid {cell_idx}/{total_cells}] mag_mu={mag_mu:.2f} | mag_sig={mag_sig:.2f}")

            # accumulators
            TP_mu = FP_mu = TP_var = FP_var = 0
            hits5_mu = hits5_var = 0
            z01_mu_sum = z01_var_sum = 0

            for i in range(trials):
                if print_progress and ((i+1) % max(1, trials//4) == 0 or i == 0 or i+1 == trials):
                    print(f"   trials: {i+1}/{trials}", end="\r")

                # Build supports once; then evaluate 2 queries (mean & var) with same supports
                supports = [build_prompt_mixed(s, n_obs, n_int, mag_mu, mag_sig,
                                               12345 + i + 100*r, prompt_type=None,
                                               prob_mean=prob_mean_supports)
                            for r in range(R)]
                q_mean = build_prompt_mixed(s, n_obs, n_int, mag_mu, mag_sig,
                                            12345 + i + 9999, prompt_type="mean")
                q_var  = build_prompt_mixed(s, n_obs, n_int, mag_mu, mag_sig,
                                            12345 + i + 9998, prompt_type="var")

                # to device
                for spt in supports:
                    for k in ["mu_obs","mu_int","Sigma_obs","Sigma_int","y"]:
                        spt[k] = spt[k].to(device_)
                for q in [q_mean, q_var]:
                    for k in ["mu_obs","mu_int","Sigma_obs","Sigma_int","y"]:
                        q[k] = q[k].to(device_)

                k_s = min(s, p)

                # ---- mean query ----
                p_q = model.forward_episode(supports, q_mean)
                true_idx = torch.nonzero(q_mean["y"], as_tuple=False).view(-1)
                true_set = set(true_idx.tolist())

                pred_idx_s = torch.topk(p_q, k=k_s).indices.tolist()
                hits_s = len(set(pred_idx_s) & true_set)
                TP_mu += hits_s
                FP_mu += (k_s - hits_s)

                k5 = min(k_top, p)
                pred_idx_5 = torch.topk(p_q, k=k5).indices.tolist()
                hits5_mu += len(set(pred_idx_5) & true_set)

                z01_mu_sum += hits_s

                details_rows.append({
                    "type": "mean",
                    "mag_mu": float(mag_mu), "mag_sig": float(mag_sig),
                    "trial": int(i),
                    "true_support": sorted(list(true_set)),
                    "pred_top_s": sorted(pred_idx_s),
                    "hits_top_s": int(hits_s),
                    "topk_idx": pred_idx_5,
                    "topk_p": p_q.detach().cpu()[pred_idx_5].tolist(),
                })

                # ---- variance query ----
                p_q = model.forward_episode(supports, q_var)
                true_idx = torch.nonzero(q_var["y"], as_tuple=False).view(-1)
                true_set = set(true_idx.tolist())

                pred_idx_s = torch.topk(p_q, k=k_s).indices.tolist()
                hits_s = len(set(pred_idx_s) & true_set)
                TP_var += hits_s
                FP_var += (k_s - hits_s)

                pred_idx_5 = torch.topk(p_q, k=k5).indices.tolist()
                hits5_var += len(set(pred_idx_5) & true_set)

                z01_var_sum += hits_s

                details_rows.append({
                    "type": "var",
                    "mag_mu": float(mag_mu), "mag_sig": float(mag_sig),
                    "trial": int(i),
                    "true_support": sorted(list(true_set)),
                    "pred_top_s": sorted(pred_idx_s),
                    "hits_top_s": int(hits_s),
                    "topk_idx": pred_idx_5,
                    "topk_p": p_q.detach().cpu()[pred_idx_5].tolist(),
                })

            if print_progress:
                print(" " * 28, end="\r")
                print(f"   ✓ finished mag_mu={mag_mu:.2f}, mag_sig={mag_sig:.2f}")

            precision_mean = TP_mu  / max(1, TP_mu + FP_mu)
            precision_var  = TP_var / max(1, TP_var + FP_var)

            summary_rows.append({
                "mag_mu": float(mag_mu),
                "mag_sig": float(mag_sig),
                "precision_mean": float(precision_mean),
                "precision_var":  float(precision_var),
                "hits_at_5_mean": float(hits5_mu / float(trials)),
                "hits_at_5_var":  float(hits5_var / float(trials)),
                "zero_one_mean":  float(z01_mu_sum / float(trials)),
                "zero_one_var":   float(z01_var_sum / float(trials)),
                "precision_avg":  float(0.5*(precision_mean + precision_var)),
                "zero_one_avg":   float(0.5*((z01_mu_sum + z01_var_sum)/float(trials))),
            })

    return pd.DataFrame(summary_rows), pd.DataFrame(details_rows)

# ----------------------------
# Run: train + eval + save
# ----------------------------
# Problem setup
p = P          # 4 variables: w,x,y,z
R = 64         # shots (supports) per episode
s = 2          # support size
n_obs = 512
n_int = 512

model = ICLSetTransformerMixedNoL(
    p=p, d_model=128, n_heads=8, d_ff=256, depth=3,
    r_sigma=16, mlp_hidden=128, dropout=0.0,
    use_isab=True, m_induce=64, max_R=128
).to(device)

print("\nTraining (BCE only, mixed mean/variance prompts)…")
model = train_bce_mixed(
    model, steps=2000, R=R, s=s, n_obs=n_obs, n_int=n_int,
    mag_mu_choices=(0.2, 0.4, 0.6, 0.8, 1.0),
    mag_sig_choices=(0.2, 0.4, 0.6, 0.8, 1.0),
    prob_mean_supports=0.5, lr=2e-4, wd=1e-4
)

print("\nEvaluating (paired mean & variance queries)…")
mag_mu_values  = [0.2, 0.4, 0.6, 0.8, 1.0]
mag_sig_values = [0.2, 0.4, 0.6, 0.8, 1.0]
summary_df, details_df = eval_grid_mixed(
    model, R=R, s=s, n_obs=n_obs, n_int=n_int,
    mag_mu_values=mag_mu_values, mag_sig_values=mag_sig_values,
    trials=30, k_top=5, prob_mean_supports=0.5, print_progress=True
)

# Save artifacts
try:
    import openpyxl  # noqa
except Exception:
    import sys, subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "openpyxl"])
    import openpyxl  # noqa

summary_df.to_csv("nl_mixed_mean_var_summary.csv", index=False)
details_df.to_csv("nl_mixed_mean_var_details.csv", index=False)
with pd.ExcelWriter("nl_mixed_mean_var_summary_and_details.xlsx", engine="openpyxl") as writer:
    summary_df.to_excel(writer, index=False, sheet_name="Summary")
    details_df.to_excel(writer, index=False, sheet_name="Details")

print("\nSaved: nl_mixed_mean_var_summary.csv, nl_mixed_mean_var_details.csv, nl_mixed_mean_var_summary_and_details.xlsx")
print("\n=== Summary (paired; mean & var) ===")
print(summary_df.sort_values(["mag_mu","mag_sig"]).to_string(index=False))
