# -*- coding: utf-8 -*-
"""SetTransformer_SEM_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZmLzpYBnKpmp7s5WyOFwwwH3GjiF5HQa

# Set Transformer for Linear SEM (Colab)
Follow these cells in order. In Colab, set **Runtime → Change runtime type → GPU** for speed.
"""

import torch, sys, platform
print('Python', sys.version)
print('PyTorch', torch.__version__)
print('CUDA available:', torch.cuda.is_available())
if torch.cuda.is_available():
    print('CUDA device:', torch.cuda.get_device_name(0))

"""## 1) Write the module file"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile sem_set_transformer.py
# 
# from dataclasses import dataclass
# from typing import Optional, Tuple, Dict
# import math
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# 
# 
# class FeedForward(nn.Module):
#     def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0):
#         super().__init__()
#         self.lin1 = nn.Linear(d_model, d_ff)
#         self.lin2 = nn.Linear(d_ff, d_model)
#         self.dropout = nn.Dropout(dropout)
# 
#     def forward(self, x):
#         x = self.lin1(x)
#         x = F.gelu(x)
#         x = self.dropout(x)
#         x = self.lin2(x)
#         x = self.dropout(x)
#         return x
# 
# 
# class MAB(nn.Module):
#     """
#     Multihead Attention Block without positional encodings (perm‑equivariant).
#     Pre‑LN residual layout for stability.
#     """
#     def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.0):
#         super().__init__()
#         self.ln1_q = nn.LayerNorm(d_model)
#         self.ln1_kv = nn.LayerNorm(d_model)
#         self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
#         self.dropout = nn.Dropout(dropout)
# 
#         self.ln2 = nn.LayerNorm(d_model)
#         self.ff = FeedForward(d_model, d_ff, dropout)
# 
#     def forward(self, X, Y):
#         # X: (B, n, d), Y: (B, m, d)
#         q = self.ln1_q(X)
#         kv = self.ln1_kv(Y)
#         out, _ = self.attn(q, kv, kv, need_weights=False)
#         X = X + self.dropout(out)
#         X = X + self.ff(self.ln2(X))
#         return X
# 
# 
# class SAB(nn.Module):
#     """Self‑Attention Block: MAB(X, X)."""
#     def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.0):
#         super().__init__()
#         self.mab = MAB(d_model, n_heads, d_ff, dropout)
# 
#     def forward(self, X):
#         return self.mab(X, X)
# 
# 
# class ISAB(nn.Module):
#     """
#     Induced Set Attention Block (Lee et al., Set Transformer).
#     Uses m learned inducing points to reduce cost for large sets.
#     """
#     def __init__(self, d_model: int, n_heads: int, d_ff: int, m: int, dropout: float = 0.0):
#         super().__init__()
#         self.I = nn.Parameter(torch.randn(1, m, d_model) / math.sqrt(d_model))
#         self.mab1 = MAB(d_model, n_heads, d_ff, dropout)
#         self.mab2 = MAB(d_model, n_heads, d_ff, dropout)
# 
#     def forward(self, X):
#         B = X.size(0)
#         I = self.I.expand(B, -1, -1)  # (B, m, d)
#         H = self.mab1(I, X)           # (B, m, d)
#         return self.mab2(X, H)        # (B, n, d)
# 
# 
# class EquivariantBackbone(nn.Module):
#     """
#     Stack of SAB/ISAB layers (perm‑equivariant).
#     """
#     def __init__(self, d_model: int, n_heads: int, d_ff: int, depth: int, dropout: float = 0.0,
#                  use_isab: bool = False, m_induce: int = 32):
#         super().__init__()
#         blocks = []
#         for _ in range(depth):
#             if use_isab:
#                 blocks.append(ISAB(d_model, n_heads, d_ff, m_induce, dropout))
#             else:
#                 blocks.append(SAB(d_model, n_heads, d_ff, dropout))
#         self.blocks = nn.ModuleList(blocks)
# 
#     def forward(self, X):
#         for blk in self.blocks:
#             X = blk(X)
#         return X
# 
# 
# class TokenEncoder(nn.Module):
#     """
#     Per‑variable token encoder.
#     Input features for variable i:
#         [ μ_obs[i], μ_int[i], Δμ[i],   PΣ Σ_row[i,:],   PL L_col[:,i] ]
#     where PΣ and PL are learned linear projections from ℝ^p → ℝ^{rΣ} and ℝ^{rL}.
#     """
#     def __init__(self, p: int, r_sigma: int, r_L: int, d_model: int, mlp_hidden: int):
#         super().__init__()
#         self.p = p
#         self.proj_sigma = nn.Linear(p, r_sigma, bias=False)
#         self.proj_Lcol = nn.Linear(p, r_L, bias=False)
#         d_in = 3 + r_sigma + r_L
#         self.mlp = nn.Sequential(
#             nn.Linear(d_in, mlp_hidden),
#             nn.GELU(),
#             nn.Linear(mlp_hidden, d_model),
#         )
# 
#     def forward(self, mu_obs: torch.Tensor, mu_int: torch.Tensor,
#                 delta_mu: torch.Tensor, Sigma: torch.Tensor, L: torch.Tensor):
#         """
#         mu_obs, mu_int, delta_mu : (p,)
#         Sigma : (p, p) (empirical covariance)
#         L     : (p, p) (lower‑triangular Cholesky)
#         Returns: tokens X of shape (1, p, d_model)
#         """
#         p = mu_obs.shape[0]
#         assert p == self.p, f"TokenEncoder was built for p={self.p}, got {p}."
# 
#         # Row i of Sigma, Column i of L
#         Sigma_rows = Sigma  # (p, p)
#         L_cols = L         # (p, p)
#         # Project each row/col
#         sigma_feat = self.proj_sigma(Sigma_rows)        # (p, r_sigma)
#         Lcol_feat = self.proj_Lcol(L_cols.T)            # (p, r_L) since passing columns as rows
# 
#         scalars = torch.stack([mu_obs, mu_int, delta_mu], dim=-1)  # (p, 3)
#         feats = torch.cat([scalars, sigma_feat, Lcol_feat], dim=-1)  # (p, 3 + rσ + rL)
#         tokens = self.mlp(feats).unsqueeze(0)  # (1, p, d_model)
#         return tokens
# 
# 
# class SemSetTransformer(nn.Module):
#     """
#     Full model: token encoder → permutation‑equivariant backbone → per‑token heads.
#     Outputs:
#         p ∈ (0,1)^p   (intervention probabilities)
#         z_hat ∈ R^p   (coefficient estimates for z*)
#     """
#     def __init__(self, p: int, d_model: int = 128, n_heads: int = 8, d_ff: int = 256,
#                  depth: int = 3, r_sigma: int = 32, r_L: int = 32, mlp_hidden: int = 128,
#                  dropout: float = 0.0, use_isab: bool = False, m_induce: int = 32):
#         super().__init__()
#         self.p = p
#         self.encoder = TokenEncoder(p, r_sigma, r_L, d_model, mlp_hidden)
#         self.backbone = EquivariantBackbone(d_model, n_heads, d_ff, depth, dropout, use_isab, m_induce)
#         self.cls_head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, 1))  # logits
#         self.reg_head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, 1))  # scalar
# 
#     def forward(self, mu_obs: torch.Tensor, mu_int: torch.Tensor,
#                 Sigma: torch.Tensor, L: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
#         delta_mu = mu_int - mu_obs
#         X = self.encoder(mu_obs, mu_int, delta_mu, Sigma, L)  # (1, p, d)
#         H = self.backbone(X)                                  # (1, p, d)
#         logits = self.cls_head(H).squeeze(0).squeeze(-1)      # (p,)
#         p = torch.sigmoid(logits)                              # (p,)
#         z_hat = self.reg_head(H).squeeze(0).squeeze(-1)       # (p,)
#         return p, z_hat
# 
# 
# @dataclass
# class LossWeights:
#     alpha: float = 1.0  # support classification
#     beta: float = 1.0   # coefficient regression
#     gamma: float = 0  # reconstruction (physics) loss
#     eta: float = 0   # sparsity regularizer
# 
# 
# def reconstruction_loss(L: torch.Tensor, z_hat: torch.Tensor, delta_mu: torch.Tensor) -> torch.Tensor:
#     """
#     L: (p, p) Cholesky of regularized covariance
#     z_hat: (p,)
#     delta_mu: (p,)
#     Returns: scalar L2 reconstruction loss || L z_hat − Δμ ||^2 / p
#     """
#     recon = L @ z_hat - delta_mu
#     return (recon.pow(2).sum() / L.shape[0])
# 
# 
# def compute_losses(p_hat: torch.Tensor, z_hat: torch.Tensor, L: torch.Tensor, delta_mu: torch.Tensor,
#                    y_true: Optional[torch.Tensor] = None, z_true: Optional[torch.Tensor] = None,
#                    weights: LossWeights = LossWeights(),
#                    sparsity_on_z: bool = True) -> Dict[str, torch.Tensor]:
#     """
#     p_hat: (p,), probabilities
#     z_hat: (p,)
#     y_true: (p,), 0/1 or None
#     z_true: (p,) or None
#     """
#     p = p_hat
#     losses = {}
#     total = torch.tensor(0.0, device=p.device)
# 
#     if y_true is not None and weights.alpha > 0:
#         bce = F.binary_cross_entropy(p.clamp(1e-6, 1-1e-6), y_true.float())
#         losses['Lsup'] = bce
#         total = total + weights.alpha * bce
# 
#     if z_true is not None and weights.beta > 0:
#         mse = F.mse_loss(z_hat, z_true)
#         losses['Lreg'] = mse
#         total = total + weights.beta * mse
# 
#     if weights.gamma > 0:
#         lrec = reconstruction_loss(L, z_hat, delta_mu)
#         losses['Lrecon'] = lrec
#         total = total + weights.gamma * lrec
# 
#     if weights.eta > 0:
#         if sparsity_on_z:
#             ls = z_hat.abs().mean()
#         else:
#             ls = p.mean()
#         losses['Lsparse'] = ls
#         total = total + weights.eta * ls
# 
#     losses['Ltotal'] = total
#     return losses
# 
# 
# # ---------- Utilities for SEM simulation and statistics ----------
# 
# def sample_random_dag_and_params(p: int, exp_degree: float, w_low: float = 0.2, w_high: float = 0.6,
#                                  sigma2_low: float = 0.5, sigma2_high: float = 1.5, seed: Optional[int] = None):
#     """
#     Returns (B (strictly lower-triangular in causal order), D (diag variances), topo_order),
#     along with (I - B)^{-1} D^{1/2} as L_star.
#     """
#     g = torch.Generator()
#     if seed is not None:
#         g.manual_seed(seed)
# 
#     # Random topological order
#     perm = torch.randperm(p, generator=g)
#     invperm = torch.argsort(perm)
# 
#     # Build strictly lower‑triangular B in causal order (perm order)
#     B_perm = torch.zeros(p, p)
#     prob = exp_degree / max(1, p - 1)
#     for i in range(p):
#         for j in range(i):  # edges j -> i allowed
#             if torch.rand(1, generator=g).item() < prob:
#                 w = (torch.rand(1, generator=g).item() * (w_high - w_low)) + w_low
#                 sign = 1.0 if torch.rand(1, generator=g).item() < 0.5 else -1.0
#                 B_perm[i, j] = sign * w
#     # Noise variances
#     sigma2 = sigma2_low + (sigma2_high - sigma2_low) * torch.rand(p, generator=g)
#     D = torch.diag(sigma2)
# 
#     # L_star in causal order: (I - B)^{-1} D^{1/2}
#     I = torch.eye(p)
#     L_star_perm = torch.linalg.solve(I - B_perm, torch.diag(torch.sqrt(sigma2)))
# 
#     # Unpermute back to original label space
#     P = torch.eye(p)[perm]           # rows perm picks
#     Pinv = torch.eye(p)[invperm]
#     B = Pinv @ B_perm @ P
#     L_star = Pinv @ L_star_perm @ P   # columns/rows permuted back
#     return B, D, perm, L_star
# 
# 
# def simulate_samples(B: torch.Tensor, D: torch.Tensor, n: int, delta: Optional[torch.Tensor] = None,
#                      seed: Optional[int] = None) -> torch.Tensor:
#     """
#     Simulate samples X ∈ R^{n×p} from linear SEM: X = (I - B)^{-1} (ε + δ)
#     where ε ~ N(0, D), δ is mean shift in exogenous noise (constant vector).
#     """
#     g = torch.Generator()
#     if seed is not None:
#         g.manual_seed(seed)
#     p = B.shape[0]
#     I = torch.eye(p)
#     A = torch.linalg.solve(I - B, I)  # (I - B)^{-1}
#     # ε ~ N(0, D)
#     eps = torch.randn(n, p, generator=g) * torch.sqrt(torch.diag(D)).unsqueeze(0)
#     if delta is None:
#         delta = torch.zeros(p)
#     # (ε + δ) then linear transform
#     X = (eps + delta.view(1, p)) @ A.T
#     return X
# 
# 
# def empirical_stats(X_obs: torch.Tensor, X_int: torch.Tensor, eps_reg: float = 1e-3) -> Tuple[torch.Tensor, ...]:
#     """
#     Compute μ̂_obs, μ̂_int, Σ̂ (obs covariance), and L̂ (Cholesky of Σ̂ + εI).
#     """
#     mu_obs = X_obs.mean(dim=0)
#     mu_int = X_int.mean(dim=0)
#     Xc = X_obs - mu_obs
#     Sigma = (Xc.T @ Xc) / max(1, X_obs.shape[0] - 1)
#     Sigma = Sigma + eps_reg * torch.eye(Sigma.shape[0])
#     try:
#         L_hat = torch.linalg.cholesky(Sigma, upper=False)
#     except RuntimeError:
#         # If numerical issues, add more jitter
#         jitter = 1e-2 * Sigma.diag().mean().item()
#         L_hat = torch.linalg.cholesky(Sigma + jitter * torch.eye(Sigma.shape[0]), upper=False)
#     return mu_obs, mu_int, Sigma, L_hat
# 
# 
# def generate_sem_batch(p: int, exp_degree: float, n_obs: int, n_int: int, s: int,
#                        delta_scale: float = 1.0, seed: Optional[int] = None):
#     """
#     Generate one synthetic SEM example with labels y (support) and z_star = D^{-1/2} δ.
#     """
#     B, D, perm, L_star = sample_random_dag_and_params(p, exp_degree, seed=seed)
#     g = torch.Generator()
#     if seed is not None:
#         g.manual_seed(seed + 1234)
# 
#     # choose intervention support S and magnitudes δ on exogenous noise
#     S_idx = torch.randperm(p, generator=g)[:s]
#     delta = torch.zeros(p)
#     delta[S_idx] = torch.randn(s, generator=g) * delta_scale
# 
#     # True z* = D^{-1/2} δ
#     z_star = delta / torch.sqrt(torch.diag(D))
# 
#     X_obs = simulate_samples(B, D, n_obs, delta=None, seed=seed)
#     X_int = simulate_samples(B, D, n_int, delta=delta, seed=(None if seed is None else seed + 1))
# 
#     mu_obs, mu_int, Sigma, L_hat = empirical_stats(X_obs, X_int, eps_reg=1e-3)
# 
#     y = torch.zeros(p)
#     y[S_idx] = 1.0
#     return mu_obs, mu_int, Sigma, L_hat, y, z_star, delta
# 
# 
# # -------------- Quick demo --------------
# 
# def _demo_train_step():
#     device = "cpu"
#     p = 32
#     mu_obs, mu_int, Sigma, L_hat, y, z_star, delta = generate_sem_batch(
#         p=p, exp_degree=3.0, n_obs=256, n_int=256, s=3, delta_scale=1.5, seed=42
#     )
#     mu_obs, mu_int, Sigma, L_hat, y, z_star = [
#         t.to(device) for t in (mu_obs, mu_int, Sigma, L_hat, y, z_star)
#     ]
# 
#     model = SemSetTransformer(p=p, d_model=128, n_heads=8, d_ff=256, depth=3,
#                               r_sigma=32, r_L=32, mlp_hidden=128, dropout=0.0,
#                               use_isab=False).to(device)
#     opt = torch.optim.Adam(model.parameters(), lr=1e-3)
# 
#     p_hat, z_hat = model(mu_obs, mu_int, Sigma, L_hat) ## remove this L_hat
#     delta_mu = mu_int - mu_obs
#     losses = compute_losses(p_hat, z_hat, L_hat, delta_mu, y_true=y, z_true=z_star,
#                             weights=LossWeights(alpha=1.0, beta=1.0, gamma=0, eta=0),
#                             sparsity_on_z=True)
# 
#     opt.zero_grad()
#     losses['Ltotal'].backward()
#     opt.step()
# 
#     return {
#         "Ltotal": losses['Ltotal'].item(),
#         "Lsup": float(losses.get('Lsup', torch.tensor(float('nan')))),
#         "Lreg": float(losses.get('Lreg', torch.tensor(float('nan')))),
#         "Lrecon": float(losses.get('Lrecon', torch.tensor(float('nan')))),
#         "Lsparse": float(losses.get('Lsparse', torch.tensor(float('nan')))),
#         "support_pred_topk": torch.topk(p_hat, k=5).indices.tolist(),
#         "support_true": torch.nonzero(y, as_tuple=False).view(-1).tolist()
#     }
# 
# 
# if __name__ == "__main__":
#     summary = _demo_train_step()
#     print("One training step summary:", summary)
#

"""## 2) Smoke test: one forward/backward step on synthetic data"""

import torch
from sem_set_transformer import SemSetTransformer, LossWeights, generate_sem_batch, compute_losses

device = 'cuda' if torch.cuda.is_available() else 'cpu'
p = 32
mu_obs, mu_int, Sigma, L_hat, y, z_star, _ = generate_sem_batch(
    p=p, exp_degree=3.0, n_obs=256, n_int=256, s=3, seed=42
)
to_dev = lambda t: t.to(device)
mu_obs, mu_int, Sigma, L_hat, y, z_star = map(to_dev, (mu_obs, mu_int, Sigma, L_hat, y, z_star))
model = SemSetTransformer(p=p, d_model=128, n_heads=8, d_ff=256, depth=3,
                          r_sigma=32, r_L=32, mlp_hidden=128, dropout=0.0,
                          use_isab=False).to(device)
opt = torch.optim.AdamW(model.parameters(), lr=1e-3)
p_hat, z_hat = model(mu_obs, mu_int, Sigma, L_hat)
delta_mu = mu_int - mu_obs
losses = compute_losses(p_hat, z_hat, L_hat, delta_mu, y_true=y, z_true=z_star,
                        weights=LossWeights(alpha=1.0, beta=1.0, gamma=5.0, eta=1e-3))
losses['Ltotal'].backward()
opt.step(); opt.zero_grad()
print({k: float(v) for k, v in losses.items()})

"""## 3) Train for a bit on synthetic SEMs (batched by accumulation)"""

import torch
from sem_set_transformer import SemSetTransformer, LossWeights, generate_sem_batch, compute_losses

device = 'cuda' if torch.cuda.is_available() else 'cpu'
p = 64
model = SemSetTransformer(p=p, d_model=128, n_heads=8, d_ff=256, depth=4,
                          r_sigma=48, r_L=48, mlp_hidden=192, dropout=0.0,
                          use_isab=False).to(device)
opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)
sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=300)
weights = LossWeights(alpha=1.0, beta=1.0, gamma=5.0, eta=1e-3)

steps = 8000  # number of training steps
bs = 8       # gradient accumulation over bs synthetic tasks per step
for t in range(steps):
    opt.zero_grad()
    total = 0.0
    for b in range(bs):
        mu_obs, mu_int, Sigma, L_hat, y, z_star, _ = generate_sem_batch(
            p=p, exp_degree=3.0, n_obs=256, n_int=256, s=3
        )
        mu_obs=mu_obs.to(device); mu_int=mu_int.to(device)
        Sigma=Sigma.to(device); L_hat=L_hat.to(device)
        y=y.to(device); z_star=z_star.to(device)
        p_hat, z_hat = model(mu_obs, mu_int, Sigma, L_hat)
        delta_mu = mu_int - mu_obs
        losses = compute_losses(p_hat, z_hat, L_hat, delta_mu, y_true=y, z_true=z_star, weights=weights)
        (losses['Ltotal']/bs).backward()
        total += float(losses['Ltotal'])
    opt.step(); sched.step()
    if (t+1) % 20 == 0:
        print(f"step {t+1:03d} | avg Ltotal={total/bs:.4f}")

"""## 4) Evaluate on a fresh sample and inspect predictions"""

import torch
from sem_set_transformer import generate_sem_batch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
p = 64
mu_obs, mu_int, Sigma, L_hat, y, z_star, _ = generate_sem_batch(p, exp_degree=3.0, n_obs=1024, n_int=1024, s=3)
mu_obs=mu_obs.to(device); mu_int=mu_int.to(device)
Sigma=Sigma.to(device); L_hat=L_hat.to(device)
y=y.to(device); z_star=z_star.to(device)
from sem_set_transformer import SemSetTransformer
model = model  # use the trained model from above
p_hat, z_hat = model(mu_obs, mu_int, Sigma, L_hat)
topk = 5
pred_idx = torch.topk(p_hat, k=topk).indices.tolist()
true_idx = torch.nonzero(y, as_tuple=False).view(-1).tolist()
print('Top-5 predicted indices:', pred_idx)
print('True support indices   :', true_idx)
print('z_hat (first 10):', z_hat[:10].detach().cpu().numpy())
print('z_hat:', z_hat.detach().cpu().numpy())

print(z_hat)