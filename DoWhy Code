# ========================== Colab-ready: Set Transformer RCA (BCE-only) ==========================
# - Nonlinear SCM (Section D.5-like) generator
# - Episode token: [mu_obs_i, mu_int_i, delta_mu_i,  (Sigma_obs @ R)[i,:]]
# - Dual-head Set Transformer: token-level (support indices) + episode-level (intervened?)
# - Fixed train total = 300 episodes; test total = 200 episodes (50% obs, 50% interventional)
# - Custom collate_fn to handle variable-length rc_indices lists without crashing
# ================================================================================================

import os, math, numpy as np, pandas as pd
import torch, torch.nn as nn, torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

# -----------------------------
# 0) Repro + device
# -----------------------------
np.random.seed(0)
torch.manual_seed(0)
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# -----------------------------
# 1) Nonlinear D.5 SCM
# -----------------------------
NODES = [
    "Website", "www", "API", "Auth Service",
    "Product DB", "Customer DB", "Order DB",
    "Caching Service", "Shipping Cost Service",
    "Product Service", "Order Service",
]
idx = {n:i for i,n in enumerate(NODES)}
p = len(NODES)

PARENTS = {
    "Website": [],
    "www":     [],
    "API":     ["Website","www"],
    "Auth Service": ["API"],
    "Product DB":   [],
    "Customer DB":  [],
    "Order DB":     [],
    "Caching Service": ["Product DB"],  # overridden by gating below
    "Shipping Cost Service": ["Order DB"],
    "Product Service": ["Shipping Cost Service","Caching Service","Customer DB"],  # overridden by max rule
    "Order Service": ["API","Product Service"],
}
TOPO_ORDER = [
    "Website", "www", "Product DB", "Customer DB", "Order DB",
    "API", "Auth Service", "Caching Service", "Shipping Cost Service",
    "Product Service", "Order Service"
]

TRUNC_EXP_NODES = ["Website","www","Order DB","Customer DB","Product DB"]
HALF_NORMAL_NODES = ["API","Auth Service","Product Service","Order Service","Shipping Cost Service","Caching Service"]

TRUNC_SCALE = {n:0.2 for n in TRUNC_EXP_NODES}
TRUNC_HIGH  = {"Website":2.0, "www":2.0, "Order DB":2.5, "Customer DB":2.0, "Product DB":2.0}
HALF_MEAN   = {"API":0.3, "Auth Service":0.2, "Product Service":0.5,
               "Order Service":0.4, "Shipping Cost Service":0.3, "Caching Service":0.3}
HALF_STD    = {"API":0.15,"Auth Service":0.10,"Product Service":0.25,
               "Order Service":0.20,"Shipping Cost Service":0.15,"Caching Service":0.15}

rng_default = np.random.default_rng

def _trunc_exp(n, scale, high, rng):
    x = rng.exponential(scale=scale, size=n)
    return np.minimum(x, high)

def _half_normal(n, mean, std, rng):
    return np.abs(rng.normal(0.0, std, size=n)) + mean

def _sample_noises(n, rng):
    eps = np.zeros((n, p))
    for node in TRUNC_EXP_NODES:
        eps[:, idx[node]] = _trunc_exp(n, TRUNC_SCALE[node], TRUNC_HIGH[node], rng)
    for node in HALF_NORMAL_NODES:
        eps[:, idx[node]] = _half_normal(n, HALF_MEAN[node], HALF_STD[node], rng)
    return eps

def simulate_d5_nonlin(n, root_indices=None, delta=2.0, seed=0):
    """
    root_indices: list/array of node indices to mean-shift (soft intervention). None => no-intervention.
    """
    rng = rng_default(seed)
    eps = _sample_noises(n, rng)
    if root_indices is not None and len(root_indices) > 0:
        eps[:, root_indices] += float(delta)

    X = np.zeros((n, p))
    local_rngs = [rng_default(rng.integers(1<<30)) for _ in range(n)]

    def cache_val(x_prod_db, eps_row, rgen):
        Z = rgen.binomial(1, 0.5)
        return Z * x_prod_db + eps_row[idx["Caching Service"]]

    def prod_service_val(x_ship, x_cache, x_cust, eps_row):
        return max(x_ship, x_cache, x_cust) + eps_row[idx["Product Service"]]

    for node in TOPO_ORDER:
        j = idx[node]
        if node == "Caching Service":
            for r in range(n):
                X[r, j] = cache_val(X[r, idx["Product DB"]], eps[r,:], local_rngs[r])
        elif node == "Product Service":
            for r in range(n):
                X[r, j] = prod_service_val(X[r, idx["Shipping Cost Service"]],
                                           X[r, idx["Caching Service"]],
                                           X[r, idx["Customer DB"]],
                                           eps[r,:])
        else:
            if PARENTS[node]:
                par_sum = np.sum(X[:, [idx[pn] for pn in PARENTS[node]]], axis=1)
            else:
                par_sum = 0.0
            X[:, j] = par_sum + eps[:, j]
    return X

# -----------------------------
# 2) Tokens with covariance-row compression
# -----------------------------
r_sigma = 32
RP = rng_default(12345).normal(0.0, 1.0/np.sqrt(p), size=(p, r_sigma)).astype(np.float32)  # fixed R

def episode_to_tokens(X_obs, X_int):
    mu_obs = X_obs.mean(axis=0)
    mu_int = X_int.mean(axis=0)
    dmu    = mu_int - mu_obs
    Sigma_obs = np.cov(X_obs, rowvar=False)     # (p,p)
    comp = Sigma_obs @ RP                        # (p, r_sigma)
    scalars = np.stack([mu_obs, mu_int, dmu], axis=1).astype(np.float32)  # (p,3)
    return np.concatenate([scalars, comp.astype(np.float32)], axis=1)     # (p, 3+r_sigma)

# -----------------------------
# 3) Episode builders + fixed splits
# -----------------------------
class RCADataset(Dataset):
    def __init__(self, episodes): self.episodes = episodes
    def __len__(self): return len(self.episodes)
    def __getitem__(self, i):
        e = self.episodes[i]
        # Keep rc_indices as a Python list to avoid variable-length stacking
        return (
            torch.from_numpy(e["tokens"]).float(),         # (p, d)
            torch.from_numpy(e["y_token"]).float(),        # (p,)
            torch.tensor(e["y_epi"], dtype=torch.float32), # scalar {0,1}
            e["rc_indices"],                                # Python list (len 0 or s)
            e["type"], e["seed"]
        )

def make_episode(n_obs, n_int, s, delta, seed, intervened: bool):
    if intervened:
        rng = rng_default(seed)
        roots = rng.choice(p, size=s, replace=False)
        Xo = simulate_d5_nonlin(n_obs, root_indices=None, delta=0.0, seed=seed)
        Xi = simulate_d5_nonlin(n_int, root_indices=roots, delta=delta, seed=seed+1)
        y_token = np.zeros(p, dtype=np.float32); y_token[roots] = 1.0
        y_epi = 1.0
        rc_idx = roots.astype(int).tolist()
        etype = "intervention"
    else:
        Xo = simulate_d5_nonlin(n_obs, root_indices=None, delta=0.0, seed=seed)
        Xi = simulate_d5_nonlin(n_int, root_indices=None, delta=0.0, seed=seed+1)
        y_token = np.zeros(p, dtype=np.float32)
        y_epi = 0.0
        rc_idx = []
        etype = "observation"
    tokens = episode_to_tokens(Xo, Xi)
    return {"tokens": tokens, "y_token": y_token, "y_epi": y_epi,
            "rc_indices": rc_idx, "type": etype, "seed": seed}

def build_splits(train_obs_count, train_int_count, test_obs_count, test_int_count,
                 n_obs, n_int, s, delta, base_seed=7):
    episodes_train = []
    episodes_test  = []

    # training: obs
    for r in range(train_obs_count):
        episodes_train.append(make_episode(n_obs, n_int, s, delta, seed=base_seed + 10_000 + r, intervened=False))
    # training: int
    for r in range(train_int_count):
        episodes_train.append(make_episode(n_obs, n_int, s, delta, seed=base_seed + 20_000 + r, intervened=True))

    # test: obs
    for r in range(test_obs_count):
        episodes_test.append(make_episode(n_obs, n_int, s, delta, seed=base_seed + 30_000 + r, intervened=False))
    # test: int
    for r in range(test_int_count):
        episodes_test.append(make_episode(n_obs, n_int, s, delta, seed=base_seed + 40_000 + r, intervened=True))

    # shuffle within splits (stable seed)
    rng = np.random.default_rng(123)
    episodes_train = [episodes_train[i] for i in rng.permutation(len(episodes_train))]
    episodes_test  = [episodes_test[i]  for i in rng.permutation(len(episodes_test))]

    return episodes_train, episodes_test

# -----------------------------
# 4) Set Transformer (dual heads)
# -----------------------------
class MAB(nn.Module):
    def __init__(self, dim_Q, dim_K, dim_V, num_heads=4, dropout=0.0):
        super().__init__()
        self.num_heads = num_heads
        self.fc_q = nn.Linear(dim_Q, dim_V, bias=False)
        self.fc_k = nn.Linear(dim_K, dim_V, bias=False)
        self.fc_v = nn.Linear(dim_K, dim_V, bias=False)
        self.fc_o = nn.Linear(dim_V, dim_V, bias=False)
        self.ln0  = nn.LayerNorm(dim_V)
        self.ln1  = nn.LayerNorm(dim_V)
        self.mlp  = nn.Sequential(nn.Linear(dim_V, dim_V*2),
                                  nn.ReLU(inplace=True),
                                  nn.Linear(dim_V*2, dim_V))
        self.dropout = nn.Dropout(dropout)
    def forward(self, Q, K):
        Qh, Kh, Vh = self.fc_q(Q), self.fc_k(K), self.fc_v(K)
        B, nq, d = Qh.shape; h = self.num_heads; dh = d // h
        def split(x): return x.view(B, -1, h, dh).transpose(1,2)
        Qm, Km, Vm = map(split, (Qh, Kh, Vh))
        Att = torch.softmax((Qm @ Km.transpose(-2,-1))/math.sqrt(dh), dim=-1)
        X = Att @ Vm
        X = X.transpose(1,2).contiguous().view(B, nq, d)
        X = self.ln0(Qh + self.dropout(self.fc_o(X)))
        X = self.ln1(X + self.mlp(X))
        return X

class SAB(nn.Module):
    def __init__(self, dim, num_heads=4, dropout=0.0):
        super().__init__()
        self.mab = MAB(dim, dim, dim, num_heads, dropout)
    def forward(self, X): return self.mab(X, X)

class SetTransformerDual(nn.Module):
    def __init__(self, d_in, d_model=128, num_heads=4, depth=3, dropout=0.0):
        super().__init__()
        self.enc = nn.Sequential(
            nn.Linear(d_in, d_model), nn.ReLU(inplace=True),
            nn.Linear(d_model, d_model), nn.ReLU(inplace=True)
        )
        self.blocks = nn.ModuleList([SAB(d_model, num_heads, dropout) for _ in range(depth)])
        self.token_head   = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, 1))   # (B,p,1)
        self.episode_head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, 1))   # after mean pool
    def forward(self, X):
        H = self.enc(X)
        for blk in self.blocks: H = blk(H)
        token_logits = self.token_head(H).squeeze(-1)       # (B,p)
        epi_feat = H.mean(dim=1)                            # (B,d_model)
        epi_logit = self.episode_head(epi_feat).squeeze(-1) # (B,)
        return token_logits, epi_logit

# -----------------------------
# 5) Custom collate (fix variable-length rc_indices)
# -----------------------------
def collate_episodes(batch):
    tokens = torch.stack([b[0] for b in batch], dim=0)  # (B, p, d)
    y_tok  = torch.stack([b[1] for b in batch], dim=0)  # (B, p)
    y_epi  = torch.stack([b[2] for b in batch], dim=0)  # (B,)
    rc_idx = [b[3] for b in batch]                      # list[list[int]]
    etype  = [b[4] for b in batch]                      # list[str]
    seed   = [b[5] for b in batch]                      # list[int]
    return tokens, y_tok, y_epi, rc_idx, etype, seed

# -----------------------------
# 6) Train & Evaluate for one split
# -----------------------------
def train_and_eval_for_split(train_eps, test_eps, epochs=20, batch_size=32, lr=3e-4,
                             d_in=None, device="cpu",
                             alpha_token=1.0, alpha_epi=1.0, s=3):
    train_ds = RCADataset(train_eps)
    test_ds  = RCADataset(test_eps)
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,
                              collate_fn=collate_episodes)
    test_loader  = DataLoader(test_ds,  batch_size=batch_size*2, shuffle=False,
                              collate_fn=collate_episodes)

    model = SetTransformerDual(d_in=d_in, d_model=128, num_heads=4, depth=3, dropout=0.0).to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)

    def train_epoch():
        model.train(); loss_sum=0.0; total=0
        for X, y_tok, y_epi, rc_idx, etype, seed in train_loader:
            X=X.to(device); y_tok=y_tok.to(device); y_epi=y_epi.to(device)
            opt.zero_grad()
            token_logits, epi_logit = model(X)
            loss = alpha_token*F.binary_cross_entropy_with_logits(token_logits, y_tok) \
                 + alpha_epi  *F.binary_cross_entropy_with_logits(epi_logit, y_epi)
            loss.backward(); opt.step()
            bs=X.size(0); total+=bs; loss_sum+=loss.item()*bs
        return loss_sum/max(1,total)

    @torch.no_grad()
    def evaluate():
        model.eval()
        rows=[]
        det_correct=det_total=0
        int_rows=0
        hits_sum = 0       # avg hits@|S|
        exact_sum = 0      # exact-match top-s
        hits5_sum = 0      # hits@5

        for X, y_tok, y_epi, rc_idx, etype, seed in test_loader:
            X=X.to(device)
            token_logits, epi_logit = model(X)
            token_probs = torch.sigmoid(token_logits)   # (B,p)
            epi_probs   = torch.sigmoid(epi_logit)      # (B,)
            epi_pred    = (epi_probs >= 0.5).long().cpu()

            for i in range(X.size(0)):
                t = etype[i]
                is_int = 1 if t=="intervention" else 0
                pred_int = int(epi_pred[i].item())
                det_correct += int(pred_int==is_int); det_total += 1

                # metrics for interventional episodes (index detection)
                if is_int==1:
                    int_rows += 1
                    true_support = rc_idx[i]  # Python list
                    k_s = min(len(true_support), p)
                    top_s = token_probs[i].topk(k=k_s).indices.cpu().tolist()
                    top5  = token_probs[i].topk(k=min(5,p)).indices.cpu().tolist()
                    overlap = len(set(top_s) & set(true_support))
                    hits_sum += overlap
                    exact_sum += int(set(top_s)==set(true_support))
                    hits5_sum += len(set(top5) & set(true_support))

                rows.append({
                    "episode_type": t,
                    "pred_intervened": bool(pred_int==1),
                    "pred_intervened_prob": float(epi_probs[i].item()),
                    "true_support": rc_idx[i],
                    "top5_pred": token_probs[i].topk(k=min(5,p)).indices.cpu().tolist()
                })

        det_acc = det_correct/max(1,det_total)         # episode-level detection accuracy
        avg_hits_s = hits_sum/max(1,int_rows)          # average overlap with true |S|
        exact_match_rate = exact_sum/max(1,int_rows)   # exact recovery rate
        hits_at_5 = hits5_sum/max(1,int_rows)          # avg hits@5
        return det_acc, avg_hits_s, exact_match_rate, hits_at_5, pd.DataFrame(rows)

    # Train
    for ep in range(1, epochs+1):
        tr_loss = train_epoch()
        if ep==1 or ep%5==0:
            det, ahs, emr, h5, _ = evaluate()
            print(f"  epoch {ep:02d} | loss={tr_loss:.4f} | det={det:.3f} | avg_hits@|S|={ahs:.2f} | exact={emr:.3f} | hits@5={h5:.2f}")

    return evaluate()

# -----------------------------
# 7) Experiment sweep
# -----------------------------
# Problem sizes for per-episode sample sizes
n_obs = 800
n_int = 800
delta = 2.0

# Fixed test sizes: total 110 = 55 obs + 55 int
test_total = 110
test_obs_count  = test_total // 2  # 55
test_int_count  = test_total // 2  # 55

# Sweep: support sizes and how many interventional episodes in training
s_values = [3, 5]
train_total = 300
train_int_values = [20, 50, 100, 150, 200]  # train_obs_count = 300 - train_int_count

d_in = 3 + r_sigma
all_summary = []
all_details = []

for s in s_values:
    print(f"\n=== Support size s={s} ===")
    for train_int_count in train_int_values:
        train_obs_count = max(0, train_total - train_int_count)
        print(f"\n-- Building split (train_total={train_total}: train_int={train_int_count}, train_obs={train_obs_count})")
        train_eps, test_eps = build_splits(
            train_obs_count=train_obs_count,
            train_int_count=train_int_count,
            test_obs_count=test_obs_count,
            test_int_count=test_int_count,
            n_obs=n_obs, n_int=n_int, s=s, delta=delta, base_seed=7+100*s+train_int_count
        )

        print(f"Train prompt: {len(train_eps)} "
              f"(obs={sum(e['y_epi']==0 for e in train_eps)}, int={sum(e['y_epi']==1 for e in train_eps)})")
        print(f"Test  prompt: {len(test_eps)}  "
              f"(obs={sum(e['y_epi']==0 for e in test_eps)}, int={sum(e['y_epi']==1 for e in test_eps)})")

        det_acc, avg_hits_s, exact_match_rate, hits_at_5, details_df = train_and_eval_for_split(
            train_eps, test_eps,
            epochs=200, batch_size=32, lr=3e-4, d_in=d_in, device=device,
            alpha_token=1.0, alpha_epi=1.0, s=s
        )

        all_summary.append({
            "s": s,
            "train_total": train_total,
            "train_int_count": train_int_count,
            "train_obs_count": train_obs_count,
            "test_total": test_total,
            "test_int_count": test_int_count,
            "test_obs_count": test_obs_count,
            "episode_detection_acc": det_acc,
            "avg_hits_at_|S|": avg_hits_s,      # 0..s
            "exact_match_rate": exact_match_rate,
            "hits_at_5": hits_at_5
        })
        df_tmp = details_df.copy()
        df_tmp["s"] = s
        df_tmp["train_total"] = train_total
        df_tmp["train_int_count"] = train_int_count
        df_tmp["train_obs_count"] = train_obs_count
        df_tmp["test_total"] = test_total
        all_details.append(df_tmp)

summary_df = pd.DataFrame(all_summary)
details_df = pd.concat(all_details, ignore_index=True)

os.makedirs("/content/outputs", exist_ok=True)
sum_path = "/content/outputs/setxf_fixedtotal_summary_200test.csv"
det_path = "/content/outputs/setxf_fixedtotal_details_200test.csv"
summary_df.to_csv(sum_path, index=False)
details_df.to_csv(det_path, index=False)

print("\n=== Sweep Summary (fixed train_total=300, test_total=200=100/100) ===")
print(summary_df.sort_values(["s","train_int_count"]).to_string(index=False))
print(f"\nSaved: {sum_path}\nSaved: {det_path}")

# To download in Colab (uncomment):
# from google.colab import files
# files.download(sum_path); files.download(det_path)
